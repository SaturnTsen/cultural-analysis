{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f73e7d",
   "metadata": {},
   "source": [
    "## Util: Text cleaner\n",
    "\n",
    "This notebook helps to clean the text data for further analysis. It uses chuking and LLM to clean the data.\n",
    "\n",
    "1. Remove irrelevant content such as tables of contents, headers, footers, and page numbers;\n",
    "2. Remove meaningless isolated numbers;\n",
    "3. Retain only complete Chinese, French or English sentences, each on a new line, ending with a period;\n",
    "4. Do not insert line breaks in the middle of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self,\n",
    "                 base_url=\"https://api.siliconflow.cn/v1\",\n",
    "                 model_name=\"Pro/deepseek-ai/DeepSeek-V3\",\n",
    "                 max_chars_per_request=8000, batch_size=50):\n",
    "        self.client = openai.OpenAI(\n",
    "            api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "        self.max_chars_per_request = max_chars_per_request\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _split_text(self, text):\n",
    "        \"\"\"超长文本切片\"\"\"\n",
    "        return [text[i:i+self.max_chars_per_request] for i in range(0, len(text), self.max_chars_per_request)]\n",
    "\n",
    "    def _stream_clean_chunk(self, text_chunk):\n",
    "        \"\"\"Stream clean a single small text chunk\"\"\"\n",
    "        prompt = f\"\"\"Please clean the following text according to these rules:\n",
    "    1. Remove irrelevant content such as tables of contents, headers, footers, and page numbers;\n",
    "    2. Remove meaningless isolated numbers;\n",
    "    3. Retain only complete Chinese, French or English sentences, each on a new line, ending with a period;\n",
    "    4. Do not insert line breaks in the middle of sentences.\n",
    "\n",
    "    Here is the raw text to be cleaned:\n",
    "    {text_chunk}\n",
    "    \"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                yield chunk.choices[0].delta.content\n",
    "\n",
    "    def clean_and_save(self, text: str, output_path: str):\n",
    "        # print(text)\n",
    "        \"\"\"使用多线程清洗文本并保存\"\"\"\n",
    "        text_chunks = self._split_text(text)\n",
    "        id = output_path.split('/')[-1].split('.')[0]\n",
    "        print(f\"正在清洗文本{id}，分为 {len(text_chunks)} 段...\")\n",
    "\n",
    "        def clean_chunk(index, chunk_text):\n",
    "            buffer = []\n",
    "            for delta in self._stream_clean_chunk(chunk_text):\n",
    "                buffer.append(delta)\n",
    "            return index, ''.join(buffer)  # 保留原始顺序\n",
    "\n",
    "        cleaned_chunks = [None] * len(text_chunks)  # 保留结果位置\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "            futures = {executor.submit(clean_chunk, idx, chunk): idx for idx, chunk in enumerate(text_chunks)}\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"清洗进度\"):\n",
    "                idx, cleaned_text = future.result()\n",
    "                cleaned_chunks[idx] = cleaned_text\n",
    "\n",
    "        print(output_path)    \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "            buffer = []\n",
    "            for cleaned_text in cleaned_chunks:\n",
    "                buffer.append(cleaned_text)\n",
    "                if len(buffer) >= self.batch_size:\n",
    "                    f_out.write(''.join(buffer))\n",
    "                    f_out.flush()\n",
    "                    buffer.clear()\n",
    "\n",
    "            if buffer:\n",
    "                f_out.write(''.join(buffer))\n",
    "                f_out.flush()\n",
    "\n",
    "        print(f\"\\n✅ 已保存到: {output_path}\")\n",
    "        \n",
    "def batch_clean_texts(text_cleaner: TextCleaner, raw_text_list, output_list, max_workers=15):\n",
    "    \"\"\"批量并发清洗，由外部控制\"\"\"\n",
    "    def worker(text, output_path):\n",
    "        try:\n",
    "            text_cleaner.clean_and_save(text, output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 处理失败: {output_path}，错误：{e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for text, output_path in zip(raw_text_list, output_list):\n",
    "            futures.append(executor.submit(worker, text, output_path))\n",
    "\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Processing files\"):\n",
    "            pass\n",
    "\n",
    "    print(\"\\n✅ 所有文件处理完毕！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "import os\n",
    "import re\n",
    "raw_path = \"path/to/your/raw/text/files\"  # Replace with your actual path\n",
    "output_path = \"path/to/your/output/files\"  # Replace with your actual path\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "raw_filenames = sorted(\n",
    "    [filename for filename in os.listdir(raw_path)\n",
    "        if filename.endswith('.txt')\n",
    "        and not filename.startswith('cleaned_')\n",
    "        and not os.path.exists(os.path.join(output_path, f\"cleaned_{filename}\"))],\n",
    "    key=natural_sort_key\n",
    ")\n",
    "\n",
    "raw_text_list = []\n",
    "for filename in raw_filenames:\n",
    "    with open(os.path.join(raw_path, filename), 'r', encoding='utf-8') as f:\n",
    "        raw_text_list.append(f.read())\n",
    "        \n",
    "output_list = [os.path.join(output_path, f\"cleaned_{filename}\") for filename in raw_filenames]\n",
    "batch_clean_texts(TextCleaner(), raw_text_list, output_list, max_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9931e",
   "metadata": {},
   "source": [
    "### Util: Translator\n",
    "\n",
    "This notebook helps to translate the text data for further analysis. It uses Standard OpenAI API to translate the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff083027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import aiofiles\n",
    "from asyncio import Semaphore\n",
    "import openai\n",
    "from tqdm.asyncio import tqdm\n",
    "from config import BASE_URL, PAYLOAD_MODEL, API_KEY\n",
    "\n",
    "class AsyncTextTranslator:\n",
    "    def __init__(self,\n",
    "                 target_lang: str,\n",
    "                 base_url: str = BASE_URL,\n",
    "                 model_name: str = PAYLOAD_MODEL,\n",
    "                 max_chars_per_request: int = 2000,\n",
    "                 batch_size: int = 50,\n",
    "                 concurrency_limit: int = 100,\n",
    "                 timeout: int = 360):\n",
    "        self.client = openai.AsyncOpenAI(\n",
    "            api_key=API_KEY,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "        self.target_lang = target_lang\n",
    "        self.max_chars_per_request = max_chars_per_request\n",
    "        self.batch_size = batch_size\n",
    "        self.semaphore = Semaphore(concurrency_limit)\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def _split_text(self, text: str) -> List[str]:\n",
    "        return [text[i:i + self.max_chars_per_request] for i in range(0, len(text), self.max_chars_per_request)]\n",
    "\n",
    "    async def _safe_stream_translate_chunk(self, chunk_text: str) -> str:\n",
    "        try:\n",
    "            return await asyncio.wait_for(self._stream_translate_chunk(chunk_text), timeout=self.timeout)\n",
    "        except asyncio.TimeoutError:\n",
    "            print(f\"[⏰ Timeout] Chunk exceeded {self.timeout}s\")\n",
    "            return \"[Translation Timeout]\"\n",
    "        except Exception as e:\n",
    "            print(f\"[❌ Error] Chunk failed: {e}\")\n",
    "            return \"[Translation Error]\"\n",
    "\n",
    "    async def _stream_translate_chunk(self, text_chunk: str) -> str:\n",
    "        system_prompt = (\n",
    "            f\"You are a translation engine. \"\n",
    "            f\"Translate all input to {self.target_lang}. Keep format, do not add extra content.\"\n",
    "        )\n",
    "        async with self.semaphore:\n",
    "            # print(f\"[💬] 正在翻译: {text_chunk[:20]}...\")\n",
    "            response = await self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": text_chunk}\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                stream=True\n",
    "            )\n",
    "            buffer = []\n",
    "            async for chunk in response:\n",
    "                if chunk.choices[0].delta.content:\n",
    "                    buffer.append(chunk.choices[0].delta.content)\n",
    "            return ''.join(buffer)\n",
    "\n",
    "    async def translate_and_save(self, text_path: Path, output_path: Path = None):\n",
    "        async with aiofiles.open(text_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f_in:\n",
    "            text = await f_in.read()\n",
    "        text_chunks = self._split_text(text)\n",
    "        if output_path is None:\n",
    "            output_path = text_path.with_suffix('.translated.txt')\n",
    "            print(f\"[📁] 翻译文件: {output_path.stem}\")\n",
    "\n",
    "        coroutines = [self._safe_stream_translate_chunk(\n",
    "            chunk) for chunk in text_chunks]\n",
    "        translated_chunks = await tqdm.gather(*coroutines, desc=f\"翻译进度: {text_path.stem}\")\n",
    "\n",
    "        async with aiofiles.open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "            buffer = []\n",
    "            for chunk in translated_chunks:\n",
    "                buffer.append(chunk)\n",
    "                if len(buffer) >= self.batch_size:\n",
    "                    await f_out.write(''.join(buffer))\n",
    "                    await f_out.flush()\n",
    "                    buffer.clear()\n",
    "            if buffer:\n",
    "                await f_out.write(''.join(buffer))\n",
    "                await f_out.flush()\n",
    "\n",
    "        # print(f\"✅ 已保存至: {output_path}\")\n",
    "\n",
    "    async def translate_all(self, text_paths: List[Path]):\n",
    "        results = await asyncio.gather(\n",
    "            *(self.translate_and_save(path) for path in text_paths),\n",
    "            return_exceptions=True\n",
    "        )\n",
    "        for r in results:\n",
    "            if isinstance(r, Exception):\n",
    "                print(\"[❌]\", repr(r))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
