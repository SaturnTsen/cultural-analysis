{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21bf517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ming\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import jieba\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "# 确保NLTK相关资源下载\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "logger = logging.getLogger('Data Preprocessor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7d70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_CONFIG = {\n",
    "    \"china\": \"zh\",\n",
    "    \"france\": \"fr\",\n",
    "    \"en\": \"en\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30841547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"数据预处理类，负责清洗和准备数据进行分析\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化数据预处理器\"\"\"\n",
    "        self.raw_data_dir: str = RAW_DATA_DIR\n",
    "        self.processed_data_dir: str = PROCESSED_DATA_DIR\n",
    "        self._lemmatizer = WordNetLemmatizer()\n",
    "        self._nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "        # 确保目录存在\n",
    "        os.makedirs(self.processed_data_dir, exist_ok=True)\n",
    "\n",
    "        # 加载停用词\n",
    "        self.stopwords = {}\n",
    "        for lang_code in LANGUAGE_CONFIG.values():\n",
    "            try:\n",
    "                self.stopwords[lang_code] = \\\n",
    "                    set(stopwords.words(self._get_nltk_language_name(lang_code))). \\\n",
    "                    union(spacy.blank(lang_code).Defaults.stop_words)\n",
    "            except:\n",
    "                raise ValueError(\n",
    "                    f\"无法加载{lang_code}的停用词，请检查NLTK数据是否完整\")\n",
    "\n",
    "    def preprocess_all(self):\n",
    "        \"\"\"预处理所有数据\"\"\"\n",
    "        logger.info(\"开始预处理所有数据\")\n",
    "\n",
    "        # 获取所有国家目录\n",
    "        country_dirs = glob(os.path.join(self.raw_data_dir, \"*\"))\n",
    "\n",
    "        for country_dir in country_dirs:\n",
    "            country = os.path.basename(country_dir)\n",
    "            logger.info(f\"预处理{country}数据\")\n",
    "\n",
    "            # 获取该国家的所有行业目录\n",
    "            sector_dirs = glob(os.path.join(country_dir, \"*\"))\n",
    "\n",
    "            for sector_dir in sector_dirs:\n",
    "                sector = os.path.basename(sector_dir)\n",
    "                logger.info(f\"预处理{country}的{sector}领域数据\")\n",
    "\n",
    "                # 创建对应的处理后数据目录\n",
    "                processed_sector_dir = os.path.join(\n",
    "                    self.processed_data_dir, country, sector)\n",
    "                if not os.path.exists(processed_sector_dir):\n",
    "                    os.makedirs(processed_sector_dir)\n",
    "\n",
    "                for txt_file in tqdm(os.listdir(sector_dir), desc=f\"处理{sector}领域数据\"):\n",
    "                    if not txt_file.endswith('.txt') or not os.path.exists(os.path.join(sector_dir, txt_file)):\n",
    "                        continue\n",
    "                    # 处理文本文件\n",
    "                    text_path = os.path.join(sector_dir, txt_file)\n",
    "                    json_file = txt_file.replace('.txt', '.json')\n",
    "                    json_path = os.path.join(sector_dir, json_file)\n",
    "                    if not os.path.isfile(json_path):\n",
    "                        continue\n",
    "\n",
    "                    # 读取对应的 metadata\n",
    "                    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                        # print(f\"正在处理元数据文件: {json_path}\")\n",
    "                        metadata = json.load(f)\n",
    "                    self._process_text_file(\n",
    "                        country, sector, text_path, processed_sector_dir, metadata)\n",
    "\n",
    "        # 生成总体元数据统计\n",
    "        self._generate_metadata_summary()\n",
    "\n",
    "        logger.info(\"所有数据预处理完成\")\n",
    "\n",
    "    def _process_text_file(self, country: Literal['france', 'china'], sector: str, text_file: Path, output_dir: Path, metadata: Dict):\n",
    "        \"\"\"处理单个文本文件\"\"\"\n",
    "        filename = os.path.basename(text_file)\n",
    "        logger.info(f\"处理文本文件: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # 读取文本\n",
    "            with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            # 检测语言\n",
    "            lang = metadata['语言']\n",
    "            # 标准化处理\n",
    "            processed_text = self._normalize_text(text, lang)\n",
    "            # 分词\n",
    "            tokens = self._tokenize_text(processed_text, lang)\n",
    "            # 词形还原\n",
    "            lemmatized_tokens = self._lemmatize_tokens(tokens, lang)\n",
    "            # 移除停用词\n",
    "            filtered_tokens = self._remove_stopwords(lemmatized_tokens, lang)\n",
    "\n",
    "            # 保存处理后的文本\n",
    "            processed_file = os.path.join(output_dir, f\"processed_words_{filename}\")\n",
    "            with open(processed_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(' '.join(filtered_tokens))\n",
    "\n",
    "            # 保存分句结果，用于后续情感分析\n",
    "            sentences = self._split_sentences(text, lang)\n",
    "            sentences_file = os.path.join(output_dir, f\"processed_sentences_{filename}\")\n",
    "            with open(sentences_file, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(sentences))\n",
    "\n",
    "            # 更新元数据\n",
    "            metadata.update({\n",
    "                'token_count': len(tokens),  # 分词后的token数量\n",
    "                'filtered_token_count': len(filtered_tokens),  # 过滤停用词后的token数量\n",
    "                'sentence_count': len(sentences),  # 分句后的句子数量\n",
    "            })\n",
    "\n",
    "            # 保存更新后的元数据\n",
    "            metadata_file = os.path.join(\n",
    "                output_dir, f\"processed_metadata_{filename.replace('.txt', '.json')}\")\n",
    "            with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            logger.info(f\"文本文件处理完成: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"处理文本文件{filename}时出错: {str(e)}\")\n",
    "\n",
    "    def _normalize_text(self, text, lang: Literal['zh', 'en', 'fr']):\n",
    "        \"\"\"标准化文本（移除特殊字符、统一大小写等）\"\"\"\n",
    "        assert lang in ['zh', 'en', 'fr'], f\"不支持的语言: {lang}\"\n",
    "\n",
    "        # 1. 移除HTML标签\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "        # 2. 移除多余的空白字符\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # 3. 对于英文和法文，转换为小写\n",
    "        if lang in ['en', 'fr']:\n",
    "            text = text.lower()\n",
    "            # 移除英文和法文的标点符号\n",
    "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "            # 移除数字\n",
    "            text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "        # 中文处理\n",
    "        elif lang == 'zh':\n",
    "            # 移除中文标点符号\n",
    "            text = re.sub(r'[^\\u4e00-\\u9fa5\\s]', ' ', text)\n",
    "            # 移除数字\n",
    "            text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "        # 4. 再次清理多余的空白字符\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "        \n",
    "    def _lemmatize_tokens(self, tokens: Literal['zh', 'en', 'fr'], lang: str) -> List[str]:\n",
    "        if lang.lower() == 'en':\n",
    "            return [self._lemmatizer.lemmatize(token) for token in tokens]\n",
    "        elif lang.lower() == 'fr':\n",
    "            doc = self._nlp_fr(\" \".join(tokens))\n",
    "            return [token.lemma_ for token in doc]\n",
    "        \n",
    "        else:\n",
    "            # 默认原样返回\n",
    "            return tokens\n",
    "\n",
    "    def _tokenize_text(self, text: str, lang: Literal['zh', 'en', 'fr']) -> List[str]:\n",
    "        \"\"\"分词\"\"\"\n",
    "        if lang in ['en', 'fr']:\n",
    "            return word_tokenize(text, language='french' if lang == 'fr' else 'english')\n",
    "        elif lang == 'zh':\n",
    "            return list(jieba.cut(text))\n",
    "        return []\n",
    "\n",
    "    def _remove_stopwords(self, tokens: List[str], lang: Literal['zh', 'en', 'fr']) -> List[str]:\n",
    "        \"\"\"移除停用词\"\"\"\n",
    "        assert lang in ['zh', 'en', 'fr'], f\"不支持的语言: {lang}\"\n",
    "\n",
    "        return [token for token in tokens if token not in self.stopwords[lang] and len(token) > 1]\n",
    "\n",
    "    def _split_sentences(self, text: str, lang: Literal['zh', 'en', 'fr']) -> List[str]:\n",
    "        \"\"\"按行分句（每行视为一个句子），忽略空行\"\"\"\n",
    "        return [line.strip() for line in text.splitlines() if line.strip()]\n",
    "\n",
    "    def _generate_metadata_summary(self):\n",
    "        \"\"\"生成元数据汇总和按国家的详细统计\"\"\"\n",
    "        # 查找所有元数据文件\n",
    "        metadata_files = glob(os.path.join(\n",
    "            self.processed_data_dir, \"**\", \"processed_metadata_*.json\"), recursive=True)    \n",
    "\n",
    "        summary_data = []   \n",
    "\n",
    "        for metadata_file in metadata_files:\n",
    "            try:\n",
    "                with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "                    metadata = json.load(f) \n",
    "\n",
    "                # 解析路径以获取国家和行业\n",
    "                parts = metadata_file.split(os.sep)\n",
    "                country_idx = parts.index(\n",
    "                    os.path.basename(self.processed_data_dir)) + 1\n",
    "                country = parts[country_idx] if country_idx < len(\n",
    "                    parts) else \"unknown\"\n",
    "                sector = parts[country_idx + 1] if country_idx + 1 < len(parts) else \"unknown\"  \n",
    "\n",
    "                summary_entry = {\n",
    "                    'id': metadata.get('id', ''),\n",
    "                    'country': country,\n",
    "                    'sector': sector,\n",
    "                    'language': metadata.get('language', 'unknown'),\n",
    "                    'token_count': metadata.get('token_count', 0),\n",
    "                    'filtered_token_count': metadata.get('filtered_token_count', 0),\n",
    "                    'sentence_count': metadata.get('sentence_count', 0),\n",
    "                    'processing_date': metadata.get('processing_date', ''),\n",
    "                    'metadata_file': metadata_file\n",
    "                }   \n",
    "\n",
    "                summary_data.append(summary_entry)  \n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"处理元数据文件{metadata_file}时出错: {str(e)}\")  \n",
    "\n",
    "        # 创建数据表\n",
    "        df = pd.DataFrame(summary_data) \n",
    "\n",
    "        # 保存汇总表\n",
    "        summary_file = os.path.join(self.processed_data_dir, \"corpus_summary.csv\")\n",
    "        df.to_csv(summary_file, index=False, encoding='utf-8')  \n",
    "\n",
    "        # 总体统计\n",
    "        stats = {\n",
    "            'total_documents': len(df),\n",
    "            'documents_by_country': df['country'].value_counts().to_dict(),\n",
    "            'documents_by_sector': df['sector'].value_counts().to_dict(),\n",
    "            'documents_by_language': df['language'].value_counts().to_dict(),\n",
    "            'total_tokens': int(df['token_count'].sum()),\n",
    "            'total_filtered_tokens': int(df['filtered_token_count'].sum()),\n",
    "            'total_sentences': int(df['sentence_count'].sum()),\n",
    "            'average_tokens_per_document': float(df['token_count'].mean()),\n",
    "            'average_sentences_per_document': float(df['sentence_count'].mean()),\n",
    "        }   \n",
    "\n",
    "        # 按国家统计详细token信息\n",
    "        token_stats_by_country = df.groupby('country').agg({\n",
    "            'token_count': 'sum',\n",
    "            'filtered_token_count': 'sum',\n",
    "            'sentence_count': 'sum',\n",
    "            'id': 'count'\n",
    "        }).rename(columns={\n",
    "            'token_count': 'total_tokens',\n",
    "            'filtered_token_count': 'total_filtered_tokens',\n",
    "            'sentence_count': 'total_sentences',\n",
    "            'id': 'document_count'\n",
    "        }).to_dict(orient='index')  \n",
    "\n",
    "        # 合并入总stats\n",
    "        stats['token_stats_by_country'] = token_stats_by_country    \n",
    "\n",
    "        # 保存统计信息\n",
    "        stats_file = os.path.join(self.processed_data_dir, \"corpus_stats.json\")\n",
    "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, ensure_ascii=False, indent=2)   \n",
    "\n",
    "        logger.info(f\"元数据汇总生成完成，总共处理了{len(df)}个文档\")\n",
    "\n",
    "    def _get_nltk_language_name(self, lang_code):\n",
    "        \"\"\"根据语言代码获取NLTK语言名称\"\"\"\n",
    "        # NLTK语言代码映射\n",
    "        nltk_language_map = {\n",
    "            'zh': 'chinese',\n",
    "            'fr': 'french',\n",
    "            'en': 'english'\n",
    "        }\n",
    "\n",
    "        return nltk_language_map[lang_code]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c855f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理education领域数据:   0%|          | 0/176 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ming\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.528 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "处理education领域数据: 100%|██████████| 176/176 [00:01<00:00, 131.23it/s]\n",
      "处理financial领域数据: 100%|██████████| 177/177 [00:04<00:00, 41.41it/s]\n",
      "处理healthcare领域数据: 100%|██████████| 176/176 [00:00<00:00, 216.33it/s]\n",
      "处理industrial领域数据: 100%|██████████| 176/176 [00:02<00:00, 75.57it/s]\n",
      "处理public_services领域数据: 100%|██████████| 176/176 [00:00<00:00, 187.15it/s]\n",
      "处理education领域数据: 100%|██████████| 176/176 [01:47<00:00,  1.64it/s]\n",
      "处理financial领域数据: 100%|██████████| 176/176 [00:58<00:00,  3.01it/s]\n",
      "处理healthcare领域数据: 100%|██████████| 176/176 [00:47<00:00,  3.72it/s]\n",
      "处理industrial领域数据: 100%|██████████| 176/176 [00:58<00:00,  3.00it/s]\n",
      "处理public_services领域数据: 100%|██████████| 176/176 [00:47<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessor = DataPreprocessor()\n",
    "preprocessor.preprocess_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad74bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
